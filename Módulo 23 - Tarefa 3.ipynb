{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55b9b7a",
   "metadata": {},
   "source": [
    "# Módulo 23 - Tarefa 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38d8fe",
   "metadata": {},
   "source": [
    "- Quais são os hyperparâmetros do Random Forest?\n",
    "- Pra que serve cada um deles?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73edb5b",
   "metadata": {},
   "source": [
    "### Hyperparâmetros do Random Forest:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a67ce",
   "metadata": {},
   "source": [
    "**n_estimators**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1cc5e",
   "metadata": {},
   "source": [
    "Este parâmetro representa o número total de árvores no modelo. Apesar do Random Forest ser capaz de gerar infinitas árvores, não existe a relação que quanto mais árvores, melhor será o desempenho do modelo. Há um ponto de equilíbrio em que a adição de mais árvores não melhora significativamente a performance e pode levar a um aumento desnecessário no tempo de treinamento.\n",
    "\n",
    "Identificar o número ideal de árvores é uma tarefa importante para quem está construindo o modelo. Geralmente, inicia-se com um valor relativamente baixo e, gradualmente, aumenta-se o número de árvores, avaliando o desempenho em um conjunto de validação. Conforme o número de árvores aumenta, a melhora na performance diminui, até que se atinge um ponto onde qualquer ganho adicional é mínimo ou até mesmo inexistente. Neste ponto, o modelo atinge sua estagnação e não é mais necessário adicionar mais árvores.\n",
    "\n",
    "O procedimento para identificar o ponto de estagnação pode ser facilitado com técnicas como validação cruzada, curvas de aprendizado e outras métricas de avaliação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1ba02",
   "metadata": {},
   "source": [
    "**criterion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bfa5b",
   "metadata": {},
   "source": [
    "Representa a escolha da função que irá medir a qualidade das separações, sendo suportados três funções, sendo elas: \"gini\", \"log_loss\" e \"entropy\". É importante salientar, que cada função tem suas próprias vantagens e desvantagens, sendo adequadas para diferentes contextos. \n",
    "\n",
    "- 1) Gini (Gini impurity):\n",
    "A função Gini mede a pureza dos nós em uma árvore de decisão. Quanto menor a impureza de um nó, mais homogêneo é o conjunto de dados que ele representa. A função Gini é eficiente computacionalmente e é uma boa escolha quando o objetivo é obter árvores mais simples e com menor profundidade. No entanto, ela pode ter tendência a criar árvores mais profundas em comparação com outras funções, o que pode levar ao overfitting.\n",
    "\n",
    "- 2) Log Loss (Logarithmic Loss):\n",
    "A função Log Loss, também conhecida como cross-entropy, é frequentemente usada em problemas de classificação binária e multiclasse, especialmente quando a saída do modelo é uma probabilidade. Ela mede a diferença entre as probabilidades preditas pelo modelo e as classes reais dos dados. Minimizar a log loss é equivalente a maximizar a probabilidade de o modelo fazer previsões corretas. É uma boa escolha quando o objetivo é obter probabilidades bem calibradas para as previsões do modelo.\n",
    "\n",
    "- 3) Entropia (Entropy):\n",
    "A entropia é uma medida de incerteza ou desordem nos dados. Quando aplicada em árvores de decisão, a entropia é usada para avaliar a homogeneidade dos nós. A função de entropia é semelhante à Gini, mas pode produzir árvores com diferentes estruturas, já que utiliza o conceito de informação mútua entre as variáveis para realizar as separações. Ela pode ser uma boa escolha em problemas onde a entropia naturalmente se relaciona com a natureza dos dados.\n",
    "\n",
    "Colinha para escolher o melhor modelo: \n",
    "- Gini: é uma boa escolha geral e eficiente computacionalmente para árvores de decisão.\n",
    "- Log_loss: se o problema for de classificação binária ou multiclasse com saída probabilística, a função pode ser preferível.\n",
    "- Entropy: quando se deseja utilizar a medida de incerteza dos dados como critério de separação. \n",
    "\n",
    "Entretanto, pode ser válido testar as três funções e ver qual retorna a melhor árvore para cada problema específico. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f02fb5",
   "metadata": {},
   "source": [
    "**max_depth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0472a",
   "metadata": {},
   "source": [
    "O hiperparâmetro \"max_depth\" no Random Forest controla a profundidade máxima que as árvores do modelo podem atingir, representando o caminho mais longo entre o nó raiz e o nó folha. Essa profundidade é crucial, pois influencia diretamente a capacidade de aprendizado e generalização do modelo.\n",
    "\n",
    "- **Observação Importante 1** Ao definir o \"max_depth\", é fundamental encontrar um equilíbrio adequado. Valores muito baixos resultarão em modelos simples demais, incapazes de capturar relações complexas nos dados e, portanto, com pouca acurácia. Em contrapartida, valores muito altos levarão a modelos com overfitting, que memorizam os dados de treinamento, mas falham em se adaptar a novos dados, reduzindo a acurácia em conjuntos de teste.\n",
    "\n",
    "- **Observação Importante 2** Para obter o desempenho ideal, é recomendável realizar ajustes interativos no valor de \"max_depth\". Começando com um valor menor, o ajuste pode ser feito ao monitorar a acurácia do modelo em um conjunto de validação à medida que o \"max_depth\" é aumentado gradualmente. O ponto em que a acurácia se estabiliza ou começa a diminuir indica o valor ótimo de \"max_depth\" que proporciona um modelo com capacidade de aprendizado adequada e generalização sólida em novos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e511520",
   "metadata": {},
   "source": [
    "**ccp_alpha**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f723b",
   "metadata": {},
   "source": [
    "O parâmetro \"ccp_alpha\" é empregado no processo de Poda com Complexidade de Custo Mínimo (Minimal Cost-Complexity Pruning). Esse método é utilizado para evitar o overfitting em árvores de decisão, tornando-as mais generalizáveis.\n",
    "\n",
    "O conceito de \"cost complexity\" mede a complexidade de uma sub-árvore. Uma sub-árvore com alta complexidade pode ser aquela que se adapta demais aos dados de treinamento e, portanto, pode ter dificuldade em generalizar para novos dados. O \"ccp_alpha\" é um valor que atua como um limiar: sub-árvores com um custo de complexidade menor que esse valor são preferidas durante o processo de poda.\n",
    "\n",
    "Ao definir o valor de \"ccp_alpha\", busca-se o equilíbrio entre complexidade e desempenho do modelo. Valores menores de \"ccp_alpha\" tendem a levar a árvores mais complexas, enquanto valores maiores favorecem árvores mais enxutas e simplificadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006f968",
   "metadata": {},
   "source": [
    "**max_samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6fa1d",
   "metadata": {},
   "source": [
    "O parâmetro \"max_samples\" é relevante quando o parâmetro \"bootstrap\" é definido como True no Random Forest. Nesse cenário, o \"max_samples\" representa a fração do conjunto de dados original que será amostrada aleatoriamente e fornecida a cada árvore individual do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf71c7",
   "metadata": {},
   "source": [
    "**min_samples_split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50542a09",
   "metadata": {},
   "source": [
    "Este parâmetro define o número mínimo de observações (amostras) necessárias em um nó para que ele possa ser dividido em sub-nós durante a construção da árvore de decisão no Random Forest.\n",
    "\n",
    "- **Informação importante:** Se esse parâmetro for definido como o valor padrão \"2\", a árvore continuará se dividindo até que cada nó seja puro, ou seja, contenha apenas amostras de uma única classe. Essa condição pode levar à criação de uma árvore extremamente complexa, que se ajusta excessivamente aos dados de treinamento, resultando em overfitting. Quando a árvore é muito complexa, ela se torna sensível aos ruídos e peculiaridades dos dados de treinamento, prejudicando sua capacidade de generalização para novos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4468167",
   "metadata": {},
   "source": [
    "**min_samples_leaf**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a72df",
   "metadata": {},
   "source": [
    "Especifica o número mínimo de amostras que devem estar presentes em um nó folha após a divisão de um nó durante a construção da árvore de decisão no Random Forest.\n",
    "\n",
    "- **Informação importante 1:** Se esse parâmetro for definido com um valor muito baixo, a árvore pode se tornar muito profunda e complexa, resultando em overfitting. Nesse caso, a árvore pode se ajustar excessivamente aos dados de treinamento, memorizando ruídos e peculiaridades específicas dos dados, mas falhando em generalizar bem para novos dados.\n",
    "\n",
    "- **Informação importante 2:** Por outro lado, se o valor de \"min_samples_leaf\" for muito alto, a árvore pode se tornar muito rasa e subutilizada, levando a underfitting. Isso significa que a árvore não se ajustará adequadamente aos dados de treinamento, resultando em um modelo que não consegue aprender com precisão os padrões presentes nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf97b6a",
   "metadata": {},
   "source": [
    "**min_weight_fraction_leaf**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8cdc3",
   "metadata": {},
   "source": [
    "A fração ponderada mínima da soma total de pesos de todas as amostras de entrada necessária para que um nó seja considerado um nó folha após a divisão.\n",
    "\n",
    "Normalmente, cada amostra de entrada tem um peso associado a ela, e esses pesos podem ser usados para ajustar a importância de cada amostra durante o treinamento do modelo. O parâmetro \"min_weight_fraction_leaf\" permite definir um limite para a quantidade total de peso necessária em um nó folha após a divisão.\n",
    "\n",
    "- **Informação importante:** Se o valor de \"min_weight_fraction_leaf\" for definido como um número muito baixo, isso pode permitir que nós folha contenham um número muito pequeno de pesos, resultando em uma árvore mais complexa e possivelmente superajustada aos dados. Por outro lado, se o valor for muito alto, os nós folha precisarão conter uma proporção significativamente maior de pesos, o que pode levar a árvores mais rasas e possivelmente subajustadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b4901",
   "metadata": {},
   "source": [
    "**max_features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83541b90",
   "metadata": {},
   "source": [
    "Número máximo de recursos (features) que são considerados para dividir cada nó durante a construção das árvores de decisão.\n",
    "\n",
    "O desempenho do modelo tende a aumentar inicialmente com o aumento do \"max_features\", pois mais informações podem ser usadas para realizar as divisões dos nós, resultando em árvores mais ricas e com maior capacidade de aprendizado. No entanto, após um ponto de saturação, adicionar mais recursos pode não trazer ganhos significativos em termos de desempenho e pode até levar a um aumento desnecessário na complexidade do modelo.\n",
    "\n",
    "- **Informação importante:** O valor padrão para \"max_features\" no modelo é geralmente a raiz quadrada do número total de recursos presentes no conjunto de dados. Esse valor é uma escolha razoável, pois permite um bom equilíbrio entre o uso de informações relevantes e a prevenção do overfitting.\n",
    "\n",
    "- **Informação ainda mais importante:** É importante observar que o valor ideal de \"max_features\" pode variar dependendo do conjunto de dados e do problema específico em questão. Portanto, realizar experimentações com diferentes valores de \"max_features\" e avaliar o desempenho do modelo em conjuntos de validação ou usando técnicas de validação cruzada é uma abordagem recomendada para encontrar o valor ótimo que resulta em um modelo bem ajustado e com boa capacidade de generalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0b600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
